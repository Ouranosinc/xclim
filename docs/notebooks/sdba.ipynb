{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Downscaling and Bias-Adjustment\n",
    "\n",
    "`xclim` provides tools and utilities to ease the bias-adjustement process through its `xclim.sdba` module. Adjustment algorithms all conform to the `train` - `adjust` scheme, formalized within `Adjustment` classes. Given a reference time series (ref), historical simulations (hist) and simulations to be adjusted (sim), any bias-adjustment method would be applied by first estimating the adjustment factors between the historical simulation and the observations series, and then applying these factors to `sim`, which could be a future simulation.\n",
    "\n",
    "A very simple \"Quantile Mapping\" approach is available through the \"Empirical Quantile Mapping\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4  # Needed for scipy.io.netcdf\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "\n",
    "# Create toy data to explore bias adjustment, here fake temperature timeseries\n",
    "t = xr.cftime_range('2000-01-01', '2030-12-31', freq='D', calendar='noleap')\n",
    "ref = xr.DataArray((-20 * np.cos(2 * np.pi * t.dayofyear / 365) + 2 * np.random.random_sample((t.size,)) + 273.15\n",
    "                    + 0.1 * (t - t[0]).days / 365),  # \"warming\" of 1K per decade,\n",
    "                   dims=('time',), coords={'time': t}, attrs={'units': 'K'})\n",
    "sim = xr.DataArray((-18 * np.cos(2 * np.pi * t.dayofyear / 365) + 2 * np.random.random_sample((t.size,)) + 273.15\n",
    "                    + 0.11 * (t - t[0]).days / 365),  # \"warming\" of 1.1K per decade\n",
    "                   dims=('time',), coords={'time': t}, attrs={'units': 'K'})\n",
    "\n",
    "ref = ref.sel(time=slice(None, '2015-01-01'))\n",
    "hist = sim.sel(time=slice(None, '2015-01-01'))\n",
    "\n",
    "ref.plot(label='Reference')\n",
    "sim.plot(label='Model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim import sdba\n",
    "\n",
    "QM = sdba.EmpiricalQuantileMapping(nquantiles=15, group='time', kind='+')\n",
    "QM.train(ref, hist)\n",
    "scen = QM.adjust(sim, extrapolation='constant', interp='nearest')\n",
    "\n",
    "ref.groupby('time.dayofyear').mean().plot(label='Reference')\n",
    "hist.groupby('time.dayofyear').mean().plot(label='Model - biased')\n",
    "scen.sel(time=slice('2000', '2015')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2000-15', linestyle='--')\n",
    "scen.sel(time=slice('2015', '2030')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2015-30', linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, a simple Quantile Mapping algorithm was used with 15 quantiles and one group of values. The model performs well, but our toy data is also quite smooth and well-behaved so this is not surprising. A more complex example could have biais distribution varying strongly across months. To perform the adjustment with different factors for each months, one can pass `group='time.month'`. Moreover, to reduce the risk of sharp change in the adjustment at the interface of the months, `interp='linear'` can be passed to `adjust` and the adjustment factors will be interpolated linearly. Ex: the factors for the 1st of May will be the average of those for april and those for may. This option is currently only implemented for monthly grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QM_mo = sdba.EmpiricalQuantileMapping(nquantiles=15, group='time.month', kind='+')\n",
    "QM_mo.train(ref, hist)\n",
    "scen = QM_mo.adjust(sim, extrapolation='constant', interp='linear')\n",
    "\n",
    "ref.groupby('time.dayofyear').mean().plot(label='Reference')\n",
    "hist.groupby('time.dayofyear').mean().plot(label='Model - biased')\n",
    "scen.sel(time=slice('2000', '2015')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2000-15', linestyle='--')\n",
    "scen.sel(time=slice('2015', '2030')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2015-30', linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data (here the adjustment factors) is available for inspection in the `ds` attribute of the adjustment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QM_mo.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QM_mo.ds.af.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "For basic time period grouping (months, day of year, season), passing a string to the methods needing it is sufficient. Most methods acting on grouped data also accept a `window` int argument to pad the groups with data from adjacent ones. Units of `window` are the sampling frequency of the main grouping dimension (usually `time`). For more complex grouping, one can pass a :py:class:`xclim.sdba.base.Grouper` directly.\n",
    "\n",
    "Example here with another, simpler, adjustment method. Here we want `sim` to be scaled so that its mean fits the one of `ref`. Scaling factors are to be computed separately for each day of the year, but including 15 days on either side of the day. This means that the factor for the 1st of May is computed including all values from the 16th of April to the 15th of May (of all years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = sdba.Grouper('time.dayofyear', window=31)\n",
    "QM_doy = sdba.Scaling(group=group, kind='+')\n",
    "QM_doy.train(ref, hist)\n",
    "scen = QM_doy.adjust(sim)\n",
    "\n",
    "ref.groupby('time.dayofyear').mean().plot(label='Reference')\n",
    "hist.groupby('time.dayofyear').mean().plot(label='Model - biased')\n",
    "scen.sel(time=slice('2000', '2015')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2000-15', linestyle='--')\n",
    "scen.sel(time=slice('2015', '2030')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2015-30', linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QM_doy.ds.af.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular approach\n",
    "\n",
    "The `sdba` module adopts a modular approach instead of implementing published and named methods directly.\n",
    "A generic bias adjustment process is laid out as follows:\n",
    "\n",
    "- preprocessing on `ref`, `hist` and `sim` (using methods in `xclim.sdba.processing` or `xclim.sdba.detrending`)\n",
    "- creating the adjustment object `Adj = Adjustment(**kwargs)` (from `xclim.sdba.adjustment`)\n",
    "- training `Adj.train(obs, sim)`\n",
    "- adjustment `scen = Adj.adjust(sim, **kwargs)`\n",
    "- post-processing on `scen` (for example: re-trending)\n",
    "\n",
    "The train-adjust approach allows to inspect the trained adjustment object. The training information is stored in the underlying `Adj.ds` dataset and always has a `af` variable with the adjustment factors. Its layout and the other available variables vary between the different algorithm, refer to their part of the API docs.\n",
    "\n",
    "Parameters needed by the training and the adjustment are saved to the `Adj.ds` dataset as a  `adj_params` attribute. Other parameters, those only needed by the adjustment are passed in the `adjust` call and written to the history attribute in the output scenario dataarray.\n",
    "\n",
    "### First example : pr and frequency adaptation\n",
    "\n",
    "The next example generates fake precipitation data and adjusts the `sim` timeseries but also adds a step where the dry-day frequency of `hist` is adapted so that is fits the one of `ref`. This ensures well-behaved adjustment factors for the smaller quantiles. Note also that we are passing `kind='*'` to use the multiplicative mode. Adjustment factors will be multiplied/divided instead of being added/substracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.random.randint(0, 1000, size=(t.size,)) / 100\n",
    "vals_ref = (4 ** np.where(vals < 9, vals/ 100, vals)) / 3e6\n",
    "vals_sim = (1 + 0.1 * np.random.random_sample((t.size,))) * (4 ** np.where(vals < 9.5, vals/ 100, vals)) / 3e6\n",
    "\n",
    "pr_ref = xr.DataArray(vals_ref, coords={\"time\": t}, dims=(\"time\",), attrs={'units': 'mm/day'})\n",
    "pr_ref = pr_ref.sel(time=slice('2000', '2015'))\n",
    "pr_sim = xr.DataArray(vals_sim, coords={\"time\": t}, dims=(\"time\",), attrs={'units': 'mm/day'})\n",
    "pr_hist = pr_sim.sel(time=slice('2000', '2015'))\n",
    "\n",
    "pr_ref.plot(alpha=0.9, label='Reference')\n",
    "pr_sim.plot(alpha=0.7, label='Model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st try without adapt_freq\n",
    "QM = sdba.EmpiricalQuantileMapping(nquantiles=15, kind='*', group='time')\n",
    "QM.train(pr_ref, pr_hist)\n",
    "scen = QM.adjust(pr_sim)\n",
    "\n",
    "pr_ref.sel(time='2010').plot(alpha=0.9, label='Reference')\n",
    "pr_hist.sel(time='2010').plot(alpha=0.7, label='Model - biased')\n",
    "scen.sel(time='2010').plot(alpha=0.6, label='Model - adjusted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, `scen` has small peaks where `sim` is 0. This problem originates from the fact that there are more \"dry days\" (days with almost no precipitation) in `hist` than in `ref`. The next example works around the problem using frequency-adaptation, as described in [Themeßl et al. (2010)](https://doi.org/10.1007/s10584-011-0224-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd try with adapt_freq\n",
    "ds_ad = sdba.processing.adapt_freq(sim=pr_hist, ref=pr_ref, thresh=0.05)\n",
    "QM_ad = sdba.EmpiricalQuantileMapping(nquantiles=15, kind='*', group='time')\n",
    "QM_ad.train(pr_ref, ds_ad.sim_ad)\n",
    "scen_ad = QM_ad.adjust(pr_sim)\n",
    "\n",
    "pr_ref.sel(time='2010').plot(alpha=0.9, label='Reference')\n",
    "pr_sim.sel(time='2010').plot(alpha=0.7, label='Model - biased')\n",
    "scen_ad.sel(time='2010').plot(alpha=0.6, label='Model - adjusted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second example: tas and detrending\n",
    "\n",
    "The next example reuses the fake temperature timeseries generated at the beginning and applies the same QM adjustment method. However, for a better adjustment, we will scale sim to ref and then detrend the series, assuming the trend is linear. When `sim` (or `sim_scl`) is detrended, its values are now anomalies, so we need to normalize `ref` and `hist` so we can compare similar values.\n",
    "\n",
    "This process is detailed here to show how the sdba module should be used in custom adjustment processes, but this specific method also exists as `sdba.DetrendedQuantileMapping` and is based on [Cannon et al. 2015](https://doi.org/10.1175/JCLI-D-14-00754.1). However, `DetrendedQuantileMapping` normalizes over a `time.dayofyear` group, regardless of what is passed in the  `group` argument. As done here, it is anyway recommended to use `dayofyear` groups when normalizing, especially for variables with strong seasonal variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_win31 = sdba.Grouper('time.dayofyear', window=15)\n",
    "Sca = sdba.Scaling(group=doy_win31, kind='+')\n",
    "Sca.train(ref, hist)\n",
    "sim_scl = Sca.adjust(sim)\n",
    "\n",
    "detrender = sdba.detrending.PolyDetrend(degree=1, group='time.dayofyear', kind='+')\n",
    "sim_fit = detrender.fit(sim_scl)\n",
    "sim_detrended = sim_fit.detrend(sim_scl)\n",
    "\n",
    "ref_n = sdba.processing.normalize(ref, group=doy_win31, kind='+')\n",
    "hist_n = sdba.processing.normalize(hist, group=doy_win31, kind='+')\n",
    "\n",
    "QM = sdba.EmpiricalQuantileMapping(nquantiles=15, group='time.month', kind='+')\n",
    "QM.train(ref_n, hist_n)\n",
    "scen_detrended = QM.adjust(sim_detrended, extrapolation='constant', interp='nearest')\n",
    "scen = sim_fit.retrend(scen_detrended)\n",
    "\n",
    "\n",
    "ref.groupby('time.dayofyear').mean().plot(label='Reference')\n",
    "sim.groupby('time.dayofyear').mean().plot(label='Model - biased')\n",
    "scen.sel(time=slice('2000', '2015')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2000-15', linestyle='--')\n",
    "scen.sel(time=slice('2015', '2030')).groupby('time.dayofyear').mean().plot(label='Model - adjusted - 2015-30', linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third example : Multi-method protocol - Hnilica et al. 2017\n",
    "In [their paper of 2017](https://doi.org/10.1002/joc.4890), Hnilica, Hanel and Puš present a bias-adjustment method based on the principles of Principal Components Analysis. The idea is simple : use principal components to define coordinates on the reference and on the simulation and then transform the simulation data from the latter to the former. Spatial correlation can thus be conserved by taking different points as the dimensions of the transform space. The method was demonstrated in the article by bias-adjusting precipitation over different drainage basins.\n",
    "\n",
    "Here we show how the modularity of `xclim.sdba` can be used to construct a quite complex adjustment protocol involving two adjustment methods : quantile mapping and principal components. Evidently, as this example uses only 2 years of data, it is not complete. It is meant to show how the adjustment functions and how the API can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using xarray's \"air_temperature\" dataset\n",
    "ds = xr.tutorial.open_dataset(\"air_temperature\")\n",
    "# To get an exagerated example we select different points\n",
    "# here \"lon\" will be our dimension of two \"spatially correlated\" points\n",
    "reft = ds.air.isel(lat=21, lon=[40, 52]).drop_vars([\"lon\", \"lat\"])\n",
    "simt = ds.air.isel(lat=18, lon=[17, 35]).drop_vars([\"lon\", \"lat\"])\n",
    "\n",
    "# Principal Components Adj, no grouping and use \"lon\" as the space dimensions\n",
    "PCA = sdba.PrincipalComponents(group=\"time\", crd_dims=['lon'])\n",
    "PCA.train(reft, simt)\n",
    "scen1 = PCA.adjust(simt)\n",
    "\n",
    "# QM, no grouping, 20 quantiles and additive adjustment\n",
    "EQM = sdba.EmpiricalQuantileMapping(group='time', nquantiles=50, kind='+')\n",
    "EQM.train(reft, scen1)\n",
    "scen2 = EQM.adjust(scen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some Analysis figures\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "gs = plt.matplotlib.gridspec.GridSpec(3, 2, fig)\n",
    "\n",
    "axPCA = plt.subplot(gs[0, :])\n",
    "axPCA.scatter(reft.isel(lon=0), reft.isel(lon=1), s=20, label='Reference')\n",
    "axPCA.scatter(simt.isel(lon=0), simt.isel(lon=1), s=10, label='Simulation')\n",
    "axPCA.scatter(scen2.isel(lon=0), scen2.isel(lon=1), s=3, label='Adjusted - PCA+EQM')\n",
    "axPCA.set_xlabel('Point 1')\n",
    "axPCA.set_ylabel('Point 2')\n",
    "axPCA.set_title('PC-space')\n",
    "axPCA.legend()\n",
    "\n",
    "refQ = reft.quantile(EQM.ds.quantiles, dim='time')\n",
    "simQ = simt.quantile(EQM.ds.quantiles, dim='time')\n",
    "scen1Q = scen1.quantile(EQM.ds.quantiles, dim='time')\n",
    "scen2Q = scen2.quantile(EQM.ds.quantiles, dim='time')\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        axQM = plt.subplot(gs[1, 0])\n",
    "    else:\n",
    "        axQM = plt.subplot(gs[1, 1], sharey=axQM)\n",
    "    axQM.plot(refQ.isel(lon=i), simQ.isel(lon=i), label='No adj')\n",
    "    axQM.plot(refQ.isel(lon=i), scen1Q.isel(lon=i), label='PCA')\n",
    "    axQM.plot(refQ.isel(lon=i), scen2Q.isel(lon=i), label='PCA+EQM')\n",
    "    axQM.plot(refQ.isel(lon=i), refQ.isel(lon=i), color='k', linestyle=':', label='Ideal')\n",
    "    axQM.set_title(f'QQ plot - Point {i + 1}')\n",
    "    axQM.set_xlabel('Reference')\n",
    "    axQM.set_xlabel('Model')\n",
    "    axQM.legend()\n",
    "\n",
    "axT = plt.subplot(gs[2, :])\n",
    "reft.isel(lon=0).plot(ax=axT, label='Reference')\n",
    "simt.isel(lon=0).plot(ax=axT, label='Unadjusted sim')\n",
    "#scen1.isel(lon=0).plot(ax=axT, label='PCA only')\n",
    "scen2.isel(lon=0).plot(ax=axT, label='PCA+EQM')\n",
    "axT.legend()\n",
    "axT.set_title('Timeseries - Point 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with dask\n",
    "\n",
    "Adjustment process can be very heavy as they are made of large amounts of small operation and often needs to be computed over large regions. Using small groupings (like `time.dayofyear`) adds precision and robustness, but also decuplates the load and computing complexity. A good first read on this are xarray's [Optimization tips](http://xarray.pydata.org/en/stable/dask.html#optimization-tips).\n",
    "\n",
    "Some additionnal tips:\n",
    "\n",
    "* When saving a file with `to_netcdf`, setting option `unlimited_dims` with a list of dimension names can force xarray to write the data using chunks on disk, instead of one contiguous array. Command-line tool `ncdump -sh` can give information on how those chunks are oriented on disk and the optimal `chunks={...}` choice can be inferred.\n",
    "* Most adjustment method will need to perform operation on the whole `time` coordinate, so it is best to optimize chunking along the other dimensions.\n",
    "* One of the main bottleneck for adjustments with small groups is that dask needs to build and optimize an enormous task graph. In order to ease that process and reduce the number of recalculations, given that the training dataset fits in memory, one could call `Adjustment.ds.load()` to trigger the computation and store the result as a `np.array`. For very large tasks, one could write the training dataset to disk and then reload them into the `Adjustment` object.\n",
    "* Consider using `engine=\"h5netcdf\"` in `open_[mf]dataset` when possible. Compatibility of a file can be found using the `is_hdf5()` method of the `h5py` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "The following script is an example of some methods useful to improve performances of a simple detrended quantile mapping adjustment. As we are using the sample data of previous example, dask isn't even needed here, and performance of this cell cannot be compared with what would happen on very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.name = 'tas'\n",
    "ref = ref.expand_dims(lon=[50, 60, 70, 80, 90, 100])\n",
    "ref.to_dataset().to_netcdf('reference_data.nc', unlimited_dims=['lon'])\n",
    "sim.name = 'tas'\n",
    "sim = sim.expand_dims(lon=[50, 60, 70, 80, 90, 100])\n",
    "sim.to_dataset().to_netcdf('simulation_data.nc', unlimited_dims=['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"tas\"\n",
    "kind = '+'\n",
    "ref_period = slice('2000', '2015')\n",
    "sim_period = slice('2000', '2030')\n",
    "file_ref = 'reference_data.nc'\n",
    "file_sim = 'simulation_data.nc'\n",
    "\n",
    "# We want the robustness of a dayofyear adjustment, but the speed of the monthly computation.\n",
    "# The compromise is to split the normalization process from the quantile mapping. Smaller groups in normalization\n",
    "# reduce the boundary effects between months but the monthly quantile mapping is almost as precise in that case.\n",
    "# This is a scientific decision that should only be taken after careful analysis of the data, it is shown here as\n",
    "# an example of compromise aimed at accelerating the computation.\n",
    "g_norm = sdba.Grouper(group='time.dayofyear', window=31)\n",
    "g_qm = sdba.Grouper(group='time.month')\n",
    "\n",
    "# # Step 1 : Normalize hist and ref\n",
    "maxlon = 6\n",
    "nlon = 2\n",
    "\n",
    "# Normalize (as resample() or groupby()) generates so many small operations that dask struggles\n",
    "# to even start the computation. Here, with large data, it is more efficient to loop of the\n",
    "# chunks and to **not** use dask. In the following two loops, \"chunks=\" is not assigned, so\n",
    "# all data is loaded ad basic numpy operations are used.\n",
    "for ilon in range(0, maxlon, nlon):\n",
    "    # Using a `with` statement when opening a file automatically closes it when we exit the context.\n",
    "    # As multiple open files can be sources of bugs, this helps the coding process\n",
    "    with xr.open_dataset(file_ref)[var].isel(lon=slice(ilon, ilon + nlon)) as ref:\n",
    "        da = sdba.processing.normalize(ref, group=g_norm, kind=kind)\n",
    "        da.name = var\n",
    "        da.to_netcdf(f\"mydqm_{ilon:03d}_refn.nc\", unlimited_dims=['lon'])\n",
    "# Hist\n",
    "for ilon in range(0, maxlon, nlon):\n",
    "    with xr.open_dataset(file_sim)[var].isel(lon=slice(ilon, ilon + nlon)) as sim:\n",
    "        da = sdba.processing.normalize(sim.sel(time=ref_period), group=g_norm, kind=kind)\n",
    "        da.name = var\n",
    "        da.to_netcdf(f\"mydqm_{ilon:03d}_histn.nc\", unlimited_dims=['lon'])\n",
    "\n",
    "# reopen the files\n",
    "# Here we specify the same chunking for all files, but different values could be used\n",
    "# depending on the operations or the data.\n",
    "ref = xr.open_dataset(file_ref, chunks={'lon': 2})[var]\n",
    "hist = xr.open_dataset(file_sim, chunks={'lon': 2})[var].sel(time=ref_period)\n",
    "refn = xr.open_mfdataset(\"mydqm_*_refn.nc\", combine='by_coords')[var]\n",
    "histn = xr.open_mfdataset(\"mydqm_*_histn.nc\", combine='by_coords')[var]\n",
    "\n",
    "# Step 2 - Empirical Quantile Mapping using the normalized data\n",
    "EQM = sdba.EmpiricalQuantileMapping(nquantiles=50, kind=kind, group=g_qm)\n",
    "EQM.train(refn, histn)\n",
    "mu_ref = g_qm.apply(\"mean\", ref)\n",
    "mu_hist = g_qm.apply(\"mean\", hist)\n",
    "# EQM.ds is simply a dataset, it can be edited in place.\n",
    "EQM.ds[\"scaling\"] = sdba.utils.get_correction(mu_hist, mu_ref, '+')\n",
    "EQM.ds.scaling.attrs.update(\n",
    "    standard_name=\"Scaling factor\",\n",
    "    description=\"Scaling factor making the mean of hist match the one of hist.\",\n",
    ")\n",
    "# We trigger the training dataset computations, it divides the workload.\n",
    "EQM.ds.load()\n",
    "\n",
    "# Step 3 - Normalize and scale sim\n",
    "with xr.open_dataset(file_sim, chunks={'lon': 2})[var] as sim:\n",
    "    sim_scl = sdba.utils.apply_correction(\n",
    "        sim,\n",
    "        sdba.utils.broadcast(EQM.ds.scaling, sim, group=g_qm, interp='linear'),\n",
    "        kind=kind\n",
    "    )\n",
    "    # For faster computation and as it makes it identical to hist, we normalize sim only with the reference period.\n",
    "    sim_norm = g_norm.apply(\"mean\", sim_scl.sel(time=ref_period))\n",
    "    sim_anom = sdba.utils.apply_correction(\n",
    "        sim_scl,\n",
    "        sdba.utils.broadcast(sdba.utils.invert(sim_norm, kind=kind), sim_scl, group=g_norm, interp='nearest'),\n",
    "        kind=kind\n",
    "    )\n",
    "    xr.Dataset(data_vars={'norm': sim_norm, 'sim': sim_anom}).to_netcdf(\"dqm_simn.nc\", unlimited_dims=['lon'])\n",
    "\n",
    "# Step 4 - Detrending\n",
    "# Detrending is one of the heaviest operations so the trend is saved in its own step.\n",
    "ds_simn = xr.open_dataset(\"dqm_simn.nc\", chunks={'lon': 2})\n",
    "polyfit = sdba.detrending.PolyDetrend(group=g_qm, degree=2)\n",
    "sim_fit = polyfit.fit(ds_simn.sim)\n",
    "trend = sim_fit.get_trend(ds_simn.sim)\n",
    "trend.name = 'trend'\n",
    "trend.to_dataset().to_netcdf(\"dqm_sim_trend.nc\", unlimited_dims=['lon'])\n",
    "\n",
    "# Step 5 - Adjustment\n",
    "with xr.open_dataset(\"dqm_sim_trend.nc\", chunks={'lon': 2}) as ds_trend:\n",
    "    # With the trend already computed, the \"private\" versions of detrend and retrend\n",
    "    # can be used to skip the trend computation.\n",
    "    sim_det = sim_fit._detrend(ds_simn.sim, ds_trend.trend)\n",
    "    sim_det.name = 'sim'\n",
    "    scen_det_anom = EQM.adjust(sim_det, interp='linear', extrapolation='constant')\n",
    "    scen_anom = sim_fit._retrend(scen_det_anom, ds_trend.trend)\n",
    "    scen = sdba.utils.apply_correction(\n",
    "        scen_anom,\n",
    "        sdba.utils.broadcast(ds_simn.norm, scen_anom, group=g_norm, interp='nearest'),\n",
    "        kind=kind\n",
    "    )\n",
    "    scen.name = var\n",
    "    scen.to_netcdf(\"dqm_scen.nc\", unlimited_dims=['lon'])\n",
    "\n",
    "# Cleanup\n",
    "ref.close()\n",
    "hist.close()\n",
    "refn.close()\n",
    "histn.close()\n",
    "ds_simn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
