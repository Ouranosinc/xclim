{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing climate indicators\n",
    "\n",
    "This notebook will get you started on the use of `xclim` to subset netCDF arrays and compute climate indicators, taking advantage of parallel processing capabilities offered by `xarray` and `dask`. \n",
    "\n",
    "`xarray` is a python package making it easy to work with n-dimensional arrays. It labels axes with their names (time, lat, lon, level) instead of indices (0,1,2,3), reducing the likelihood of bugs and making the code easier to understand. One of the key strengths of `xarray` is that it knows how to deal with non-standard calendars (I'm looking at you 360_days) and can easily resample daily time series to weekly, monthly, seasonal or annual periods.  Finally, `xarray` is tightly inegrated with `dask`, a package that can automatically parallelize operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/DEV/anaconda3/envs/python36/lib/python3.6/site-packages/xarray/core/options.py:50: FutureWarning: The enable_cftimeindex option is now a no-op and will be removed in a future version of xarray.\n",
      "  FutureWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XCLIM and xarray\n",
    "import xclim.indices as indices\n",
    "import xclim.atmos as atmos\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "# file handling libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Output folder\n",
    "outfolder = Path(tempfile.mkdtemp()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Dask client - Parallel processing / workers\n",
    "\n",
    "First we create a pool of workers that will wait for jobs. The `xarray` library will automatically connect to these workers and and dispatch them jobs that can be run in parallel. \n",
    "\n",
    "The dashboard link lets you see in real time how busy those workers are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:asyncio:Using selector: EpollSelector\n",
      "DEBUG:asyncio:Using selector: EpollSelector\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46612\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8788/status' target='_blank'>http://127.0.0.1:8788/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>20</li>\n",
       "  <li><b>Memory: </b>12.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:46612' processes=2 cores=20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client\n",
    "client=Client(n_workers=2, threads_per_worker=10, dashboard_address=8788, memory_limit='6GB') \n",
    "#client=Client(n_workers=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "infolder = '<path_to_data>/cb-oura-1.0/'\n",
    "\n",
    "# Get list of files for tasmax\n",
    "rcps = ['rcp45','rcp85']\n",
    "v = 'tasmax'\n",
    "r = rcps[0]\n",
    "search_str = os.path.join(infolder, '{v}*CanESM*{r}*.nc'.format(v=v,r=r))\n",
    "sim_files= sorted(glob.glob(search_str))\n",
    "print(len(sim_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating xarray datasets\n",
    "\n",
    "To open a netCDF file with `xarray`, we use `xr.open_dataset(<path to file>)`. But by default, the entire file is stored in one chunk, so there is no parallelism. To trigger parallel computations, we need to explicitly specify the *chunk* size. \n",
    "\n",
    "`Dask`' parallelism is based on memory chunks. We need to tell `xarray` to split our netCDF array into chunks of a given size, and operations on each chunk of the array will automatically be dispatched to the workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'tasmax' (time: 365, lat: 700, lon: 1064)>\n",
      "[271852000 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 1950-12-31 00:00:00\n",
      "  * lat      (lat) float32 83.28931 83.20598 83.12265 ... 25.12497 25.04164\n",
      "  * lon      (lon) float32 -141.04314 -140.9598 ... -52.54667 -52.46334\n",
      "Attributes:\n",
      "    units:          K\n",
      "    long_name:      air_temperature\n",
      "    standard_name:  air_temperature\n"
     ]
    }
   ],
   "source": [
    "# This file is opened as one big chunk: no parallel processing. \n",
    "ds = xr.open_dataset(sim_files[0])\n",
    "print(ds.tasmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'tasmax' (time: 365, lat: 700, lon: 1064)>\n",
      "dask.array<shape=(365, 700, 1064), dtype=float32, chunksize=(31, 700, 1064)>\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 1950-12-31 00:00:00\n",
      "  * lat      (lat) float32 83.28931 83.20598 83.12265 ... 25.12497 25.04164\n",
      "  * lon      (lon) float32 -141.04314 -140.9598 ... -52.54667 -52.46334\n",
      "Attributes:\n",
      "    units:          K\n",
      "    long_name:      air_temperature\n",
      "    standard_name:  air_temperature\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 24), (700,), (1064,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunked in memory along the time dimension.\n",
    "# Note that the data type is a 'dask.array'. xarray will automatically use client workers \n",
    "ds = xr.open_dataset(sim_files[0], chunks={'time': 31})\n",
    "print(ds.tasmax)\n",
    "ds.tasmax.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multifile dataset\n",
    "netCDF files are often split into periods to keep file size manageable. A single dataset can be split in dozens of individual files. `xarray` has a function `open_mfdataset` that can open and aggregate a list of files and construct a unique *logical* dataset. `open_mfdataset` can aggregate files over coordinates (time, lat, lon) and variables. \n",
    "\n",
    "Note that opening a multi-file dataset automatically chunks the array (one chunk per file).\n",
    "\n",
    "Note also that because `xarray` reads every file metadata to place it in a logical order, it can take a while to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 700, lon: 1064, time: 55115)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 83.28931 83.20598 83.12265 ... 25.12497 25.04164\n",
      "  * lon      (lon) float32 -141.04314 -140.9598 ... -52.54667 -52.46334\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(55115, 700, 1064), chunksize=(365, 100, 112)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "# Create multi-file data & chunks \n",
    "ds = xr.open_mfdataset(sim_files, chunks={'time':365, 'lat':50*2, 'lon':56*2})\n",
    "ds = ds.drop('time_vectors')\n",
    "ds = ds.drop('ts')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subsetting utilities\n",
    "\n",
    "### subset_bbox : using a latitude-longitude bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 180, lon: 324, time: 55115)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(55115, 180, 324), chunksize=(365, 20, 75)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "from xclim import subset\n",
    "lat_bnds = [45, 60]\n",
    "lon_bnds = [-55, -82]\n",
    "\n",
    "ds1 = subset.subset_bbox(ds,lat_bnds=lat_bnds,lon_bnds=lon_bnds)\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add start and/or end years\n",
    "\n",
    "Note that in the next release, we'll use datetime objects instead of a year integer to specify start and end points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 180, lon: 324, time: 10950)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "  * time     (time) object 1981-01-01 00:00:00 ... 2010-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(10950, 180, 324), chunksize=(365, 20, 75)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n",
      " \n",
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 700, lon: 1064, time: 10950)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 83.29 83.21 83.12 83.04 ... 25.29 25.21 25.12 25.04\n",
      "  * lon      (lon) float64 -141.0 -141.0 -140.9 -140.8 ... -52.63 -52.55 -52.46\n",
      "  * time     (time) object 1981-01-01 00:00:00 ... 2010-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(10950, 700, 1064), chunksize=(365, 100, 112)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "ds2 = subset.subset_bbox(ds,lat_bnds=lat_bnds,lon_bnds=lon_bnds, start_yr=1981, end_yr=2010)\n",
    "print(ds2)\n",
    "print(' ')\n",
    "\n",
    "# subset years only\n",
    "ds2 = subset.subset_bbox(ds, start_yr=1981, end_yr=2010)\n",
    "print(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a single grid point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 43800)\n",
      "Coordinates:\n",
      "    lat      float32 50.04064\n",
      "    lon      float32 -69.96264\n",
      "  * time     (time) object 1981-01-01 00:00:00 ... 2100-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time) float32 dask.array<shape=(43800,), chunksize=(365,)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "lon_pt = -70.0\n",
    "lat_pt = 50.0\n",
    "\n",
    "ds3 = subset.subset_gridpoint(ds,lon=lon_pt,lat=lat_pt, start_yr=1981)\n",
    "print(ds3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nothing has been computed so far !\n",
    "\n",
    "If you look at the output of those operations, they're identified as `dask.array` objects. What happens is that `dask` creates a chain of operations that when executed, will yield the values we want. But as long as we don't explicitly ask for a value, no computation will occur. \n",
    "\n",
    "You can trigger computations by using the `load` or `compute` method, or writing the output to disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Climate index calculation & resampling frequencies\n",
    "\n",
    "`xclim` has two layers for the calculation of indicators. The bottom layer is composed of a list of functions that take a `xarray.DataArray` as an input and return an `xarray.DataArray` as output. You'll find these functions in `xclim.indices`. The indicator's logic is contained in this function, as well as potential unit conversions, but it doesn't check if the time frequency is daily, and doesn't not adjust the meta data of the output array. \n",
    "\n",
    "The second layer are class instances that you'll find organized by *realm*. So far, there is only one realm (atmospheric) available in `xclim.atmos`, but we'll be working on `ice` and `land` indicators in 2020. Before running computations, these classes check the input data is a daily average of the expected variable: \n",
    "1. If an indicator expects a daily mean and you pass it a daily max, a `warning` will be raised. \n",
    "2. After the computation, it also checks the number of values per period to make sure there are not missing values or `NaN` in the input data. If there are, the output is going to be set to `NaN`. \n",
    "3. The output units are set correctly as well as other properties of the output array, complying as much as possible with CF conventions. \n",
    "\n",
    "For new users, we suggest you use the classes found in `xclim.atmos`. If you know what you're doing and you want to circumvent the built-in checks, then you can use the `xclim.indices` directly. \n",
    "\n",
    "All `xclim` indicators convert daily data to lower time frequencies, such as monthly or annual values. This is done using `xarray.DataArray.resample` method. Resampling creates a grouped object over which you apply a reduction operation (e.g. mean, min, max). The list of available frequency is given in the link below, but the most often used are: \n",
    "\n",
    "- YS: annual starting in January\n",
    "- YS-JUL: annual starting in July\n",
    "- MS: monthly\n",
    "- QS-DEC: seasonal starting in December\n",
    "- 7D: 7 day (weekly)\n",
    "\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases  \n",
    "Note - not all offsets in the link are supported by cftime objects in `xarray`\n",
    "\n",
    "\n",
    "In the example below, we're computing the **annual maximum temperature of the daily maximum temperature (tx_max)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time-steps using freq ==  YS  :  151 \n",
      "\n",
      "<xarray.DataArray 'time' (time: 151)>\n",
      "array([cftime.DatetimeNoLeap(1950, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1951, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1952, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1953, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1954, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1955, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1956, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1957, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1958, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1959, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1960, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1961, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1962, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1963, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1964, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1965, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1966, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1967, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1968, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1969, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1970, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1971, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1972, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1973, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1974, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1975, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1976, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1977, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1978, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1979, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1980, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1981, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1982, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1983, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1984, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1985, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1986, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1987, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1988, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1989, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1990, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1991, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1992, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(1993, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(1994, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(1995, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(1996, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(1997, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(1998, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(1999, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2000, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2001, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2002, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2003, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2004, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2005, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2006, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2007, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2008, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2009, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2010, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2011, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2012, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2013, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2014, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2015, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2016, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2017, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2018, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2019, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2020, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2021, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2022, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2023, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2024, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2025, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2026, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2027, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2028, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2029, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2030, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2031, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2032, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2033, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2034, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2035, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2036, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2037, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2038, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2039, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2040, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2041, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2042, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2043, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2044, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2045, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2046, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2047, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2048, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2049, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2050, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2051, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2052, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2053, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2054, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2055, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2056, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2057, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2058, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2059, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2060, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2061, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2062, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2063, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2064, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2065, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2066, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2067, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2068, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2069, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2070, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2071, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2072, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2073, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2074, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2075, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2076, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2077, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2078, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2079, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2080, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2081, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2082, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2083, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2084, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2085, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2086, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2087, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2088, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2089, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2090, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2091, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2092, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2093, 1, 1, 0, 0, 0, 0, 0, 1),\n",
      "       cftime.DatetimeNoLeap(2094, 1, 1, 0, 0, 0, 0, 1, 1),\n",
      "       cftime.DatetimeNoLeap(2095, 1, 1, 0, 0, 0, 0, 2, 1),\n",
      "       cftime.DatetimeNoLeap(2096, 1, 1, 0, 0, 0, 0, 3, 1),\n",
      "       cftime.DatetimeNoLeap(2097, 1, 1, 0, 0, 0, 0, 4, 1),\n",
      "       cftime.DatetimeNoLeap(2098, 1, 1, 0, 0, 0, 0, 5, 1),\n",
      "       cftime.DatetimeNoLeap(2099, 1, 1, 0, 0, 0, 0, 6, 1),\n",
      "       cftime.DatetimeNoLeap(2100, 1, 1, 0, 0, 0, 0, 0, 1)], dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "fr = 'YS'\n",
    "ds1.tasmax.attrs['cell_methods'] = 'time: maximum within days'\n",
    "out = atmos.tx_max(ds1.tasmax, freq=fr)\n",
    "print('Number of time-steps using freq == ', fr, ' : ', len(out.time),'\\n')\n",
    "print(out.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Example output using `atmos` vs `indices` modules\n",
    "The `atmos` module adds CF metadata attributes to the output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output atmos : \n",
      " <xarray.DataArray 'txgt_25 C' (time: 151, lat: 180, lon: 324)>\n",
      "dask.array<shape=(151, 180, 324), dtype=float64, chunksize=(1, 20, 75)>\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-01-01 00:00:00\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "Attributes:\n",
      "    units:          days\n",
      "    history:        [2019-05-14 11:43:52] txgt_25 C(tasmax, thresh='25.0 degC...\n",
      "    cell_methods:   time: maximum within days time: maximum within days time:...\n",
      "    abstract:       Number of days where daily maximum temperature exceed a t...\n",
      "    keywords:       \n",
      "    standard_name:  number_of_days_with_air_temperature_above_threshold\n",
      "    long_name:      Number of days with Tmax > 25 CC\n",
      "    description:    Annual number of days where daily maximum temperature exc...\n",
      "    comment:        \n",
      "    references:     \n",
      "    notes:          \\nLet :math:`TX_{ij}` be the daily maximum temperature at...\n",
      "    parameters:     {'tasmax': {'default': None, 'desc': ''}, 'thresh': {'def... \n",
      "\n",
      "\n",
      "\n",
      "output indices : \n",
      " <xarray.DataArray 'tasmax' (time: 151, lat: 180, lon: 324)>\n",
      "dask.array<shape=(151, 180, 324), dtype=int64, chunksize=(1, 20, 75)>\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-01-01 00:00:00\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "Attributes:\n",
      "    units:    days\n"
     ]
    }
   ],
   "source": [
    "out30d = atmos.tx_days_above(ds1.tasmax,thresh='25 C',freq=fr)\n",
    "print('output atmos : \\n', out30d,'\\n\\n\\n',)\n",
    "\n",
    "out30d = indices.tx_days_above(ds1.tasmax,thresh='25 C',freq=fr)\n",
    "print('output indices : \\n', out30d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 180, lon: 324, time: 151)\n",
      "Coordinates:\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-01-01 00:00:00\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "Data variables:\n",
      "    tx_max   (time, lat, lon) float32 dask.array<shape=(151, 180, 324), chunksize=(1, 20, 75)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "# We have created an xarray data-array - We can insert this into an output dataset object\n",
    "# Create an xarray dataset object - copy original dataset global attrs\n",
    "dsOut = xr.Dataset(data_vars=None, coords=out.coords, attrs=ds1.attrs)\n",
    "# Add our climate index as a data variable to the dataset\n",
    "dsOut[out.name] = out\n",
    "print(dsOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 7. xclim computations are *lazy*\n",
    "\n",
    "Up until now we have ony created a schedule of tasks with a small preview, not done any actual computations. As mentionned above, writing the output to disk will trigger the cascade of computations on all the chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation took  147.816965341568 s\n"
     ]
    }
   ],
   "source": [
    "outfile = outfolder / 'test_tx_max.nc'\n",
    "start= time.time()\n",
    "dsOut.to_netcdf(outfile, format='NETCDF4')\n",
    "end = time.time()\n",
    "print('calculation took ',end-start, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Optimizing the chunk size\n",
    "\n",
    "You can improve performance by being smart about chunk sizes. If chunks are too small, there is a lot of time lost in overhead. If chunks are too large, you may end up exceeding the individual worker memory limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 180, lon: 324, time: 55115)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(55115, 180, 324), chunksize=(365, 20, 75)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "print(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 180, lon: 324, time: 55115)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 59.96 59.87 59.79 59.71 ... 45.29 45.21 45.12 45.04\n",
      "  * lon      (lon) float64 -81.96 -81.88 -81.8 -81.71 ... -55.21 -55.13 -55.05\n",
      "  * time     (time) object 1950-01-01 00:00:00 ... 2100-12-31 00:00:00\n",
      "Data variables:\n",
      "    tasmax   (time, lat, lon) float32 dask.array<shape=(55115, 180, 324), chunksize=(365, 180, 324)>\n",
      "Attributes:\n",
      "    Conventions:     CF-1.5\n",
      "    title:           CanESM2 model output prepared for CMIP5 historical\n",
      "    history:         2011-04-14T00:21:01Z CMOR rewrote data to comply with CF...\n",
      "    institution:     CCCma (Canadian Centre for Climate Modelling and Analysi...\n",
      "    source:          CanESM2 2010 atmosphere: CanAM4 (AGCM15i, T63L35) ocean:...\n",
      "    redistribution:  Redistribution prohibited. For internal use only.\n"
     ]
    }
   ],
   "source": [
    "ds1 = ds1.chunk(chunks={'time':365, 'lon':-1, 'lat':-1})\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation took  84.12525868415833 s\n"
     ]
    }
   ],
   "source": [
    "out = atmos.tx_max(ds1.tasmax, freq=fr)\n",
    "dsOut = xr.Dataset(data_vars=None, coords=out.coords, attrs=ds1.attrs)\n",
    "dsOut[out.name] = out\n",
    "\n",
    "start= time.time()\n",
    "\n",
    "dsOut.to_netcdf( outfile,format='NETCDF4')\n",
    "\n",
    "end = time.time()\n",
    "print('calculation took ',end-start, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XCLIM unit handling \n",
    "\n",
    "A lot of effort has been placed into automatic handling of input data units.  `xclim` will automatically detect the input variable(s) units (e.g. °C versus °K or mm/s versus mm/day etc.) and adjust on-the-fly in order to calculate indices in the consistent manner.  This comes with the obvious caveat that input data requires metadata attribute for units\n",
    "\n",
    "In the example below, we compute weekly total precipitation in mm using inputs of mm/s and mm/d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 9.66575590e-06 3.50769289e-04 1.34260035e-05\n",
      " 9.91609704e-06 0.00000000e+00 0.00000000e+00 5.53925656e-06\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.05151732e-04 1.17095777e-04\n",
      " 1.53630954e-05 4.42854389e-06 6.87591637e-06 5.46419369e-06\n",
      " 8.52907669e-06 1.59427127e-05 2.47019962e-05 0.00000000e+00\n",
      " 1.95580651e-05 3.90298010e-05 1.75301684e-05] \n",
      "\n",
      "[ 0.          0.          0.          0.          0.          0.83512133\n",
      " 30.306467    1.1600066   0.8567507   0.          0.          0.4785918\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  9.08511    10.117075    1.3273714   0.38262618  0.59407914  0.47210634\n",
      "  0.7369122   1.3774505   2.1342525   0.          1.689817    3.3721747\n",
      "  1.5146066 ]\n"
     ]
    }
   ],
   "source": [
    "dsPr = xr.open_dataset(sim_files[0].replace('tasmax','pr'),chunks={'time':31}).drop(['ts','time_vectors'])\n",
    "dsPr = subset.subset_gridpoint(dsPr,lon=lon_pt,lat=lat_pt)\n",
    "\n",
    "# Create a copy of the data converted to mm d-1\n",
    "dsPr_mmd = dsPr.copy()\n",
    "dsPr_mmd['pr'].values = dsPr.pr.values * 3600 *24\n",
    "dsPr_mmd.pr.attrs['units'] = 'mm d-1'\n",
    "\n",
    "print(dsPr.pr.values[0:31],'\\n')\n",
    "print(dsPr_mmd.pr.values[0:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. results using inputs in mm/s : \n",
      "\n",
      " units : mm \n",
      " [ 66.44051   54.20339   36.376373 101.53389   33.530552  87.39966\n",
      " 104.95978  133.99432   89.70896   56.47059   75.21128   49.83245 ] \n",
      "\n",
      "2. results using inputs in mm/d : \n",
      "\n",
      " units : mm \n",
      " [ 66.44052   54.203384  36.376373 101.5339    33.53055   87.39966\n",
      " 104.959785 133.99434   89.70897   56.470596  75.21129   49.83245 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "out1 = atmos.precip_accumulation(dsPr.pr,freq='MS')\n",
    "print('1. results using inputs in mm/s : \\n\\n','units :', out1.units,'\\n',out1.values,'\\n')\n",
    "\n",
    "out2 = atmos.precip_accumulation(dsPr_mmd.pr,freq='MS')\n",
    "print('2. results using inputs in mm/d : \\n\\n','units :', out2.units,'\\n',out2.values,'\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold indices\n",
    "\n",
    "`xclim` unit handling also applies to threshold indicators. Users can provide threshold in units of choice and `xclim` will adjust automatically.  For example determining the number of days with tasmax > 20°C users can define a threshold input of '20 C' or '20 degC' even if input data is in Kelvin.  Alernatively users could send provide a threshold in Kelvin '293.15 K' (if they really wanted to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273.5242  271.8584  247.72333 252.86673 264.26947 273.6165  274.6312\n",
      " 266.9602  249.18898 255.07526 263.65112 265.15714 253.79906 260.2527\n",
      " 261.9956  267.89328 268.66718 268.61023 268.0569  267.94894 256.55618\n",
      " 248.05493 247.59552 248.5298  248.72592 248.59483 250.02737 247.43605\n",
      " 249.71368 250.91469 249.64064] \n",
      "\n",
      "[  0.37420654  -1.2915955  -25.426666   -20.283264    -8.880524\n",
      "   0.4664917    1.4812012   -6.189789   -23.961014   -18.074738\n",
      "  -9.498871    -7.992859   -19.350937   -12.897308   -11.154388\n",
      "  -5.256714    -4.4828186   -4.5397644   -5.093109    -5.20105\n",
      " -16.593811   -25.095062   -25.554474   -24.620193   -24.424072\n",
      " -24.55516    -23.12262    -25.713943   -23.43631    -22.235306\n",
      " -23.509354  ]\n"
     ]
    }
   ],
   "source": [
    "# Original data in Kelvin\n",
    "dsTasmax = xr.open_dataset(sim_files[0],chunks={'time':31}).drop(['ts','time_vectors'])\n",
    "dsTasmax.tasmax.attrs['cell_methods'] = 'time: maximum within days'\n",
    "dsTasmax = subset.subset_gridpoint(dsTasmax,lon=lon_pt,lat=lat_pt)\n",
    "\n",
    "# Create a copy of the data converted to C\n",
    "dsTasmax_C = dsTasmax.copy()\n",
    "dsTasmax_C['tasmax'].values = dsTasmax.tasmax.values - 273.15\n",
    "dsTasmax_C.tasmax.attrs['units'] = 'C'\n",
    "\n",
    "print(dsTasmax.tasmax.values[0:31],'\\n')\n",
    "print(dsTasmax_C.tasmax.values[0:31])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. results using inputs in °K  : threshold in °C: \n",
      "\n",
      " [ 0.  0.  0.  0.  4. 14. 16. 16.  3.  0.  0.  0.] \n",
      "\n",
      "2. results using inputs in °C : threshold in °C\n",
      "\n",
      " [ 0.  0.  0.  0.  4. 14. 16. 16.  3.  0.  0.  0.]\n",
      "\n",
      "3. results using inputs in °C : threshold in °K : \n",
      "\n",
      " [ 0.  0.  0.  0.  4. 14. 16. 16.  3.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Using Kelvin data\n",
    "out1 = atmos.tx_days_above(dsTasmax.tasmax,thresh='20 C', freq='MS')\n",
    "print('1. results using inputs in °K  : threshold in °C: \\n\\n',out1.values,'\\n')\n",
    "\n",
    "# Using Celsius data\n",
    "out2 = atmos.tx_days_above(dsTasmax_C.tasmax,thresh='20 C', freq='MS')\n",
    "print('2. results using inputs in °C : threshold in °C\\n\\n',out2.values)\n",
    "\n",
    "# Using Celsius but with threshold in Kelvin\n",
    "out3 = atmos.tx_days_above(dsTasmax_C.tasmax,thresh='293.15 K', freq='MS')\n",
    "print('\\n3. results using inputs in °C : threshold in °K : \\n\\n',out3.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
